---
title: "Practical Machine Learning. Course Project"
date: "November 21, 2015"
output: html_document
---

## Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://example.comhttp://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).


```{r}
setwd("D:\\Projects\\Education\\Coursera\\Practical Machine Learning\\Course Project\\Practical-Machine-Learning")

# labeled data. contains both X and Y
labeled_data = read.csv("Data\\Src\\pml-training.csv")
# target X values. will be used to submit predictions
x_target = read.csv("Data\\Src\\pml-testing.csv")

dim(labeled_data)
```

So we have 19622 observations of  160 variables. Let's drop columns containing *only* NA values in the data we will use to submit the predictions.

```{r}
columns_to_drop = names(x_target)[colSums(is.na(x_target)) == nrow(x_target)]
x_target = x_target[ , -which(names(x_target) %in% columns_to_drop)]
labeled_data = labeled_data[ , -which(names(labeled_data) %in% columns_to_drop)]

dim(labeled_data)
```
As we can see there are 60 columns left.
Now let's drop columns that probably not affecting the prediction.
```{r}
columns_to_drop = c("X", "raw_timestamp_part_1", "raw_timestamp_part_2",
										"cvtd_timestamp","cvtd_timestamp", "problem_id")

x_target = x_target[ , -which(names(x_target) %in% columns_to_drop)]
labeled_data = labeled_data[ , -which(names(labeled_data) %in% columns_to_drop)]

dim(labeled_data)
```
The factor columns **user_name** and **new_window** should be binarized in order to be used as a variable. After this we can split the labeled data to train and test sets. The test set will be used for the final evaluation of the model. The train set will be used to used to select the best model.

```{r}
suppressWarnings(library(caret))

# binarize factor variables
y = labeled_data$classe
dummies = dummyVars(classe ~ ., labeled_data)

# label column is required for binarization
x_target$classe = NA
levels(x_target$new_window) = levels(labeled_data$new_window)

labeled_data = data.frame(predict(dummies, labeled_data))
labeled_data$classe = y

x_target = data.frame(predict(dummies, x_target))

# split to train and test
inTrain = createDataPartition(y=y, p=0.8, list=FALSE)

# will be used for trainig and cross validation
train_data = labeled_data[inTrain,]
# will be used for final evaluation
test_data = labeled_data[-inTrain,]

```

Now we can train a model
```{r include=FALSE}
train_control <- trainControl(method="cv", number=4)

model = train(classe~., data=train_data, trControl=train_control, 
							method="rf", preProcess="pca")
```

```{r}
model$finalModel
```
In sample error rate: 1.91

Let's now estimate the Out of sample error on our test data
```{r}
# make predictions
predictions <- predict(model, test_data)
confusionMatrix(predictions, test_data$classe)

```
**Out of sample** Accuracy : 98.27%

And finaly generating predictions for target data
```{r}
# make predictions
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

answers = as.character(predict(model, x_target))
pml_write_files(answers)

```